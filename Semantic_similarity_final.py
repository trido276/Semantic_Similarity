# -*- coding: utf-8 -*-
"""Final - draft

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tNcEAHGEJZo9sVV894mMZU77Be52D9Bc

# Data Processing
"""

import pandas as pd
import csv
import numpy as np
import pickle
import re
np.seterr(divide='ignore', invalid='ignore')
from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

!ls "/content/gdrive/My Drive/For_study/Final/Code/Dataset"

#Loading word2vec
from gensim.test.utils import common_texts, get_tmpfile
path = get_tmpfile("word2vec.model")
# model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
# model.save("word2vec.model")

GDRIVE = "/content/gdrive/My Drive/For_study/Final/Code/Model/"
GLOVE_1 = "glove.6B.300d.txt"
GLOVE_2 = "glove.42B.300d.txt"
GLOVE_3 = "glove.840B.300d.txt"
GLOVE_4 = "glove.twitter.27B.200d.txt"

SEMEVAL_FOUR = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/2014-task-10.txt"
SEMEVAL_FIVE_TRIAL = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-trial.txt"
SEMEVAL_FIVE_TRAIN = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-train.txt"
SEMEVAL_FIVE_TEST = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-test.txt"

def glove(location):
    #Loading Glove
    print("Loading Glove Model from "+location)
    f = open(location, encoding="utf8")
    model = {}
    for line in f:
        splitLine = line.split()
        x = splitLine[0]
        try:
            embedding = np.array([float(val) for val in splitLine[1:]])
            model[x] = embedding
        except:
            print("Error")
    print("Done.", len(model), " words loaded!")
    return model

# glove1 = glove(GDRIVE+GLOVE_1)
glove2 = glove(GDRIVE+GLOVE_2)
# glove3 = glove(GDRIVE+GLOVE_3)
# glove4 = glove(GDRIVE+GLOVE_4)

GLOVE_SHAPE = 300
#Handling data
def handleData(lines, gloveModel):
  handledData = []
  handledSentences = 0
  for line in lines:
    oneSplitSentence = splitSentence(line)
    Q = handleSentence(oneSplitSentence[0],gloveModel)
    A = handleSentence(oneSplitSentence[1],gloveModel)
    P = oneSplitSentence[2]
    fullSentence = np.append(Q,(np.append(A,P)))
    handledData.append(fullSentence)
    return handledData


#Spliting Sentence
def splitSentence(str):
    splitSentence = str.lower().split('\t')
    newSentence = ""
    try:
        splitSentence[0] = re.sub(r'[^\w+\_+]', ' ', splitSentence[0])
        splitSentence[1] = re.sub(r'[^\w+\_+]', ' ', splitSentence[1])
        splitSentence[2] = float(splitSentence[2].replace(("\n"), ""))
        newSentence=(np.array(splitSentence[0]),np.array(splitSentence[1]),np.array(splitSentence[2],dtype = float))
    except:
        pass
    return np.asarray(newSentence)

#Handling Sentence
def handleSentence(str,gloveModel):
    fullSentence = [0]
    wordCount = 0
    for word in str.split(" "): #space
        try: 
            fullSentence = fullSentence + gloveModel[word]
            wordCount = wordCount +1
        except:
            fullSentence = fullSentence + np.random.randn(1,GLOVE_SHAPE)
            wordCount = wordCount +1
            pass
    if (wordCount==0):return fullSentence
    return fullSentence/wordCount
  
#Reading data
def readData(str):
    print("Reading data "+str)
    f = open(str, encoding="utf8")
    lines = f.readlines()
    print("Done.", len(lines), " lines loaded!")
    return lines
#WritingData
def writeData(str,data):
    print("Writing " + str)
    #using csv
    """with open(str, 'w') as f:
        wr = csv.writer(f,quoting=csv.QUOTE_ALL)
        #wr = csv.writer(f)
        wr.writerows(map(lambda x: [x], data))
        #wr.writerows(data)"""
    #using pandas
    pd.DataFrame(data).to_csv(str, index=False, header=True)
    print("Writing finish!")
#Handling 
def handleDataset(path, gloveModel, outputPath):
  data = readData(path)
  handledData = handleData(data, gloveModel)
  writeData(outputPath,handledData)

def checkIfEnoughTabs(path):
  data = readData(path)
  for i in range(len(data)):
    sen = data[i].lower().split('\t')
    if (len(sen) < 3):
      print(i)
      print(sen)

# handleDataset(SEMEVAL_FOUR,glove1,GDRIVE+"semeval_4_glove1.csv")
# checkIfEnoughTabs(SEMEVAL_FIVE_TRIAL)

SEMEVAL_FOUR_GLOVE1 = "/content/gdrive/My Drive/For_study/Final/Code/Model/2014-task-10-glove1.csv"
SEMEVAL_FIVE_TRIAL_GLOVE1 = "/content/gdrive/My Drive/For_study/Final/Code/Model/2015-task-2-trial-glove1.csv"
SEMEVAL_FIVE_TRAIN_GLOVE1 ="/content/gdrive/My Drive/For_study/Final/Code/Model/2015-task-2-train-glove1.csv"
SEMEVAL_FIVE_TEST_GLOVE1 = "/content/gdrive/My Drive/For_study/Final/Code/Model/2015-task-2-test-glove1.csv"

"""# Training Glove Model"""

import keras
from keras.models import Sequential, Model
from keras.layers import *
from keras.layers import merge
from keras.utils.np_utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.initializers import Constant
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import tensorflow as tf
import pandas as pd
import csv
import numpy as np
import pickle
import re
import matplotlib.pyplot as plt
# %matplotlib inline


np.seterr(divide='ignore', invalid='ignore')
pd.set_option('display.max_colwidth', -1)
from keras import backend
backend.set_image_dim_ordering('tf')

np.random.seed(1)

def filter(data,defaultLen):
    for item in data: #space
        if (len(item)<defaultLen):
            data = data.remove(item)
    return data

def writeData(str,data):
    print("Writing " + str)
    #using pandas
    pd.DataFrame(data).to_csv(str,sep='\n', index=False, header=False, encoding='utf-8')
    print("Writing finish!")

replace_puncts = {'`': "'", '′': "'", '“':'"', '”': '"', '‘': "'"}

strip_chars = [',', '"', ':', ')', '(', '-', '|', ';', "'", '[', ']', '>', '=', '+', '\\', '•',  '~', '@', 
 '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', 
 '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', 
 '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', 
 '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]

puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']

def clean_str(x):
    x = str(x)
    
    x = x.lower()
    
    x = re.sub(r"(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})", "url", x)
    
    for k, v in replace_puncts.items():
        x = x.replace(k, f' {v} ')
        
    for punct in strip_chars:
        x = x.replace(punct, ' ') 
    
    for punct in puncts:
        x = x.replace(punct, f' {punct} ')
        
    x = x.replace(" '", " ")
    x = x.replace("' ", " ")
        
    return x

# semFourGlove1Data = pd.read_csv(SEMEVAL_FOUR_GLOVE1,delimiter=",")
# semFiveGlove1TrialData = pd.read_csv(SEMEVAL_FIVE_TRIAL_GLOVE1,delimiter=",")
# semFiveGlove1TestData = pd.read_csv(SEMEVAL_FIVE_TEST_GLOVE1,delimiter=",")
# semFiveGlove1TrainData = pd.read_csv(SEMEVAL_FIVE_TRAIN_GLOVE1,delimiter=",")
# f = open(SEMEVAL_FOUR,"r").readlines()
# f[:]=[clean_str(x.replace('\n','')) for x in f]
# format=['sen1\tsen2\tsimilarity']
# writeData(GDRIVE+"test.tsv",np.array(format+f))
d = pd.read_csv(GDRIVE+"test.tsv",delimiter='\t')
d=d[['sen1','sen2','similarity']]
d.head(3)

d_0 = d[d['similarity'] < 1].sample(frac=1)
d_1 = d[d['similarity'] < 2].sample(frac=1)
d_2 = d[d['similarity'] < 3].sample(frac=1)
d_3 = d[d['similarity'] < 4].sample(frac=1)
d_4 = d[d['similarity'] < 5].sample(frac=1)

#balanced set for training
sample_size = min(len(d_0), len(d_1), len(d_2), len(d_3), len(d_4))
data = pd.concat([d_0.head(sample_size), d_1.head(sample_size), d_2.head(sample_size), d_3.head(sample_size), d_4.head(sample_size)]).sample(frac=1)

data['l'] = (data['sen1']).apply(lambda x: len(str(x).split(' ')))
print("mean length of sentence: " + str(data.l.mean()))
print("max length of sentence: " + str(data.l.max()))
print("std dev length of sentence: " + str(data.l.std()))
sequence_length = data.l.max()

glove=glove2
max_features = 20000

tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')
tokenizer.fit_on_texts(data['sen1'].values+data['sen2'].values)
x = tokenizer.texts_to_sequences(data['sen1'].values+data['sen2'].values)
x = pad_sequences(x, sequence_length)
y = pd.get_dummies(data[['similarity']]).values
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)
print("test set size " + str(len(X_test)))
y.head(3)

pd.get_dummies(data[['similarity']]).values
# data['similarity'].head(3)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

num_words = min(max_features, len(word_index)) + 1
print(num_words)

embedding_dim = 300

# first create a matrix of zeros, this is our embedding matrix
embedding_matrix = np.zeros((num_words, embedding_dim))

# for each word in out tokenizer lets try to find that work in our w2v model
for word, i in word_index.items():
    if i > max_features:
        continue
    embedding_vector = glove.get(word)
    if embedding_vector is not None:
        # we found the word - add that words vector to the matrix
        embedding_matrix[i] = embedding_vector
    else:
        # doesn't exist, assign a random vector
        embedding_matrix[i] = np.random.randn(embedding_dim)

model = Sequential()
model.add(Embedding(num_words,
                    embedding_dim,
                    embeddings_initializer=Constant(embedding_matrix),
                    input_length=sequence_length,
                    trainable=True))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))
model.add(Bidirectional(CuDNNLSTM(32)))
model.add(Dropout(0.25))
model.add(Dense(units=5, activation='softmax'))
model.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

batch_size = 10
history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=1, validation_split=0.1)

y_hat = model.predict(X_test)
accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))
conf = confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))

y_hat

# split into input (X) and output (Y) variables
def splitData(data, shape):
  splitSentences = np.asarray(data.iloc[:,0:shape*2])
  splitPoint = np.asarray(data.iloc[:,shape*2])
  splitSentences.reshape((-1, shape))
  filter(splitSentences,shape)
  splitPoint.reshape((-1, 1))
  return np.array(splitSentences),np.array(splitPoint)
  #filter(Y,1)
  #Y=Y.astype(int)

# semFourGlove1 = splitData(semFourGlove1Data, 600)
# semFiveGlove1Test = splitData(semFiveGlove1TestData, 600)
# semFiveGlove1Train = splitData(semFiveGlove1TrainData, 600)

batch_size=1000
hidden_nodes = 1024
epochs = 4
# # create model
# def evaluate(trainModel, testModel,modelName, shape):
#   model = Sequential()
#   model.add(Dense(shape, input_dim=shape, activation='tanh'))
#   model.add(Dense(10, activation='tanh'))
#   model.add(Dense(2, activation='softmax'))
#   # Compile model
#   #categorical_crossentropy sparse_categorical_crossentropy
#   model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#   # Fit the model
#   model.fit(trainModel[0], trainModel[1], epochs=150, batch_size=10)
#   # evaluate the model
#   scores = model.evaluate(testModel[0], testModel[1])
#   print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
# #   model.save(GDRIVE+modelName+".h5")
#   return model
  
# create model
def evaluate2(trainModel, testModel,modelName, shape):
  model = Sequential()
  model.add(Dense(600, input_dim=shape, activation='tanh'))
  model.add(Dense(300, activation='tanh'))
  model.add(Dense(350, activation='tanh'))
  model.add(Dense(400, activation='tanh'))
  model.add(Dense(250, activation='tanh'))
  model.add(Dense(100, activation='tanh'))
  model.add(Dense(10, activation='softmax'))
  # Compile model
  #categorical_crossentropy
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  # Fit the model
  model.fit(trainModel[0], trainModel[1], epochs=150, batch_size=10,validation_data=(testModel[0], testModel[1]))
  # evaluate the model
  scores = model.evaluate(testModel[0], testModel[1])
  print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
  model.save(GDRIVE+modelName+".h5")
  return model
# create model
def evaluate3(trainModel, testModel, modelName, shape):
  model = Sequential()
  model.add(Dense(600, input_dim=shape, activation='tanh'))
  model.add(Dense(1200, activation='tanh'))
  model.add(Dense(700, activation='tanh'))
  model.add(Dense(550, activation='tanh'))
  model.add(Dense(300, activation='tanh'))
  model.add(Dense(350, activation='tanh'))
  model.add(Dense(200, activation='tanh'))
  model.add(Dense(250, activation='tanh'))
  model.add(Dense(150, activation='tanh'))
  model.add(Dense(100, activation='tanh'))
  model.add(Dense(50, activation='tanh'))
  model.add(Dense(10, activation='softmax'))
  # Compile model
  #categorical_crossentropy
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'],validation_data=(testModel[0], testModel[1]))
  # Fit the model
  model.fit(trainModel[0], trainModel[1], epochs=150, batch_size=10)
  # evaluate the model
  scores = model.evaluate(testModel[0], testModel[1])
  print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
  model.save(GDRIVE+modelName+".h5")
  return model

shapeGlove2 = 600
def evaluate(trainModel, testModel,modelName, shape):
  model = Sequential()
  max_features = len(trainModel[0])
  maxlen = shape
  inp = Input(shape=(maxlen,))

  x = Embedding(max_features, shape, weights=[np.array(trainModel[0])])(inp)
  x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)
  x = Dropout(0.25)(x)
#   model.add(Dropout(0.5))
  x = Dense(3, activation="sigmoid")(x)
  model = Model(inputs=inp, outputs=x)
#   model.add(Dense(1, activation='softmax'))

  # try using different optimizers and different optimizer configs
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  X_t = pad_sequences(trainModel[0], maxlen=maxlen)
  Y_t = trainModel[1]
  print('Train...')
  model.fit(X_t, Y_t, batch_size=batch_size, epochs=epochs)
  scores = model.evaluate(testModel[0], testModel[1])
  print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
  #   model.save(GDRIVE+modelName+".h5")
  return model
evaluate(semFiveGlove2Train,semFiveGlove2Test,"glove2eval1",  shapeGlove2)

# evaluate(semFiveGlove2Train+semFiveGlove2Trial+semFourGlove2,semFiveGlove2Test,"glove2eval1",  shapeGlove2)
# evaluate(semFiveGlove2Train,semFiveGlove2Test,"glove2eval1",  shapeGlove2)
len(semFiveGlove2Train[0][0])

def createData(dataTxtPath, gloveModel, dataCsvPath, shape):
  handleDataset(dataTxtPath, gloveModel, dataCsvPath)
  input_shape=(100, shape)
  dataset = pd.read_csv(dataCsvPath,delimiter=",")
  return splitData(dataset, shape)

glove2 = glove(GDRIVE+GLOVE_2)

"""**GLOVE 2**"""

#Create glove 2 data
shapeGlove2 = 300
dir = "/content/gdrive/My Drive/For_study/Final/Code/Model/"
createData(SEMEVAL_FOUR, glove2 ,dir+"2014-task-10-glove2.csv",shapeGlove2)
createData(SEMEVAL_FIVE_TRIAL , glove2,dir+"2015-trial-glove2.csv",shapeGlove2)
createData(SEMEVAL_FIVE_TRAIN , glove2,dir+"2015-train-glove2.csv",shapeGlove2)
createData(SEMEVAL_FIVE_TEST, glove2,dir+"2015-test-glove2.csv",shapeGlove2)

"""SEMEVAL_FOUR = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/2014-task-10.txt"

---
SEMEVAL_FIVE_TRIAL = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-trial.txt"

---

SEMEVAL_FIVE_TRAIN = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-train.txt"

---

SEMEVAL_FIVE_TEST = "/content/gdrive/My Drive/For_study/Final/Code/Dataset/semeval-2015-task2-test.txt"
"""

semFourGlove2 = splitData(pd.read_csv("/content/gdrive/My Drive/For_study/Final/Code/Model/2014-task-10-glove2.csv",delimiter=","),shapeGlove2)
semFiveGlove2Train = splitData(pd.read_csv("/content/gdrive/My Drive/For_study/Final/Code/Model/2015-train-glove2.csv",delimiter=","),shapeGlove2)
semFiveGlove2Test = splitData(pd.read_csv("/content/gdrive/My Drive/For_study/Final/Code/Model/2015-test-glove2.csv",delimiter=","),shapeGlove2)
semFiveGlove2Trial = splitData(pd.read_csv("/content/gdrive/My Drive/For_study/Final/Code/Model/2015-trial-glove2.csv",delimiter=","),shapeGlove2)

evaluate(semFiveGlove2Train,semFiveGlove2Test,"glove2eval1",  shapeGlove2)
# evaluate2(semFiveGlove2Train,semFiveGlove2Test,"glove2eval2", shapeGlove2)

len(semFiveGlove2Train[0][0])

evaluate3(semFiveGlove2Train,semFiveGlove2Test,"glove2eval3", shapeGlove2)

"""# Training Word2Vec Model"""

from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import word2vec
import numpy

num_features = 300  # Word vector dimensionality
min_word_count = 2 # Minimum word count
num_workers = 4     # Number of parallel threads
context = 10        # Context window size
downsampling = 1e-3 # (0.001) Downsample setting for frequent words
gdrive =  "/content/gdrive/My Drive/For_study/Final/Code/Model"

model = Word2Vec(readData(SEMEVAL_FOUR), size=100, window=5, min_count=1, workers=4)
# common_texts
model.train(readData(SEMEVAL_FIVE_TRAIN), total_examples=1, epochs=150)
model.save(GDRIVE+"word2vec.model")

# sentences = readData(SEMEVAL_FOUR)
sentences = glove
print("Training model....", len(sentences))
model = word2vec.Word2Vec(sentences, workers=num_workers,\
                          size=num_features,\
                          min_count=min_word_count,\
                          window=context,
                          sample=downsampling)

# To make the model memory efficient
model.init_sims(replace=True)
model.train(sentences, total_examples=1, epochs=150)

# Saving the model for later use. Can be loaded using Word2Vec.load()
model_name = gdrive+"glove1.h5"
model.save(model_name)

# model.wv.most_similar("dog")
# model.wv.wmdistance('hot', 'popular')

model = glove
numpy.dot(model['spain'], model['france'])/(numpy.linalg.norm(model['spain'])* numpy.linalg.norm(model['france']))
model.accuracy('/tmp/questions-words.txt')

D = np.zeros((len(docs), len(docs)))
for i in range(len(docs)):
    for j in range(len(docs)):
        if i == j:
            continue  # self-distance is 0.0
        if i > j:
            D[i, j] = D[j, i]  # re-use earlier calc
        D[i, j] = model.wmdistance(docs[i], docs[j])

sen  =['sen1','sen2','point']
# model.accuracy(SEMEVAL_FOUR)
# sentences = readData(SEMEVAL_FOUR)
# model = Word2Vec(sentences, size=200)
model.__contains__("")